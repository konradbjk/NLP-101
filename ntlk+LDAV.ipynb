{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from collections import defaultdict\n",
    "#from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sample_text.txt', 'r') as f:\n",
    "    text=f.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In January, Google launched a new service called Cloud AutoML, which can automate some tricky aspects of designing machine-learning software.',\n",
       " \"While working on this project, the company's researchers sometimes needed to run as many as 800 graphics chips in unison to train their powerful algorithms.Unlike humans, who can recognize coffee cups from seeing one or two examples, AI networks based on simulated neurons need to see tens of thousands of examples in order to identify an object.\",\n",
       " 'Imagine trying to learn to recognize every item in your environment that way, and you begin to understand why AI software requires so much computing power.If researchers could design neural networks that could be trained to do certain tasks using only a handful of examples, it would \"upend the whole paradigm\" Charles Bergan, vice president of engineering at Qualcomm, told the crowd at MIT Technology Review\\'s EmTech China conference earlier this week.If neural networks were to become capable of \"one-shot learning\" Bergan said, the cumbersome process of feeding reams of data into algorithms to train them would be rendered obsolete.',\n",
       " \"This could have serious consequences for the hardware industry, as both existing tech giants and startups are currently focused on developing more powerful processors designed to run today's data-intensive AI algorithms.It would also mean vastly more efficient machine learning.\",\n",
       " 'While neural networks that can be trained using small data sets are not a reality yet, research is already being done on making algorithms smaller without losing accuracy, Bill Dally, chief scientist at Nvidia, said at the conference.Nvidia researchers use a process called network pruning to to make a neural network smaller and more efficient to run by removing the neurons that do no contribute directly to output.',\n",
       " '\"There are ways of training that can reduce the complexity of training by huge amounts\" Dally said.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # needs punkt package\n",
    "\n",
    "sent_words = sent_tokenize(text)\n",
    "sent_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "Now we are spliting the text to words. In the first example we are going to use simple tokinizer. It splits the words. It might look trivial, like using .split('.') function and for many sentences it is. However spliting the words is also a challenge, especially when you face things like concatenations like ```we are``` and ```we're```. NLTK is going ahead and save your time with this seemingly simple, yet complex operations.\n",
    "\n",
    "In the next step there is implemented tokenizer which will also exclude the puntuations from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'january',\n",
       " ',',\n",
       " 'google',\n",
       " 'launched',\n",
       " 'a',\n",
       " 'new',\n",
       " 'service',\n",
       " 'called',\n",
       " 'cloud',\n",
       " 'automl',\n",
       " ',',\n",
       " 'which',\n",
       " 'can',\n",
       " 'automate',\n",
       " 'some',\n",
       " 'tricky',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'designing',\n",
       " 'machine-learning',\n",
       " 'software',\n",
       " '.',\n",
       " 'while',\n",
       " 'working',\n",
       " 'on',\n",
       " 'this',\n",
       " 'project',\n",
       " ',',\n",
       " 'the',\n",
       " 'company',\n",
       " \"'s\",\n",
       " 'researchers',\n",
       " 'sometimes',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'run',\n",
       " 'as',\n",
       " 'many',\n",
       " 'as',\n",
       " '800',\n",
       " 'graphics',\n",
       " 'chips',\n",
       " 'in',\n",
       " 'unison',\n",
       " 'to',\n",
       " 'train',\n",
       " 'their',\n",
       " 'powerful',\n",
       " 'algorithms.unlike',\n",
       " 'humans',\n",
       " ',',\n",
       " 'who',\n",
       " 'can',\n",
       " 'recognize',\n",
       " 'coffee',\n",
       " 'cups',\n",
       " 'from',\n",
       " 'seeing',\n",
       " 'one',\n",
       " 'or',\n",
       " 'two',\n",
       " 'examples',\n",
       " ',',\n",
       " 'ai',\n",
       " 'networks',\n",
       " 'based',\n",
       " 'on',\n",
       " 'simulated',\n",
       " 'neurons',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'tens',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'an',\n",
       " 'object',\n",
       " '.',\n",
       " 'imagine',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'to',\n",
       " 'recognize',\n",
       " 'every',\n",
       " 'item',\n",
       " 'in',\n",
       " 'your',\n",
       " 'environment',\n",
       " 'that',\n",
       " 'way',\n",
       " ',',\n",
       " 'and',\n",
       " 'you',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'why',\n",
       " 'ai',\n",
       " 'software',\n",
       " 'requires',\n",
       " 'so',\n",
       " 'much',\n",
       " 'computing',\n",
       " 'power.if',\n",
       " 'researchers',\n",
       " 'could',\n",
       " 'design',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'that',\n",
       " 'could',\n",
       " 'be',\n",
       " 'trained',\n",
       " 'to',\n",
       " 'do',\n",
       " 'certain',\n",
       " 'tasks',\n",
       " 'using',\n",
       " 'only',\n",
       " 'a',\n",
       " 'handful',\n",
       " 'of',\n",
       " 'examples',\n",
       " ',',\n",
       " 'it',\n",
       " 'would',\n",
       " '``',\n",
       " 'upend',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'paradigm',\n",
       " \"''\",\n",
       " 'charles',\n",
       " 'bergan',\n",
       " ',',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'of',\n",
       " 'engineering',\n",
       " 'at',\n",
       " 'qualcomm',\n",
       " ',',\n",
       " 'told',\n",
       " 'the',\n",
       " 'crowd',\n",
       " 'at',\n",
       " 'mit',\n",
       " 'technology',\n",
       " 'review',\n",
       " \"'s\",\n",
       " 'emtech',\n",
       " 'china',\n",
       " 'conference',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'week.if',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'were',\n",
       " 'to',\n",
       " 'become',\n",
       " 'capable',\n",
       " 'of',\n",
       " '``',\n",
       " 'one-shot',\n",
       " 'learning',\n",
       " \"''\",\n",
       " 'bergan',\n",
       " 'said',\n",
       " ',',\n",
       " 'the',\n",
       " 'cumbersome',\n",
       " 'process',\n",
       " 'of',\n",
       " 'feeding',\n",
       " 'reams',\n",
       " 'of',\n",
       " 'data',\n",
       " 'into',\n",
       " 'algorithms',\n",
       " 'to',\n",
       " 'train',\n",
       " 'them',\n",
       " 'would',\n",
       " 'be',\n",
       " 'rendered',\n",
       " 'obsolete',\n",
       " '.',\n",
       " 'this',\n",
       " 'could',\n",
       " 'have',\n",
       " 'serious',\n",
       " 'consequences',\n",
       " 'for',\n",
       " 'the',\n",
       " 'hardware',\n",
       " 'industry',\n",
       " ',',\n",
       " 'as',\n",
       " 'both',\n",
       " 'existing',\n",
       " 'tech',\n",
       " 'giants',\n",
       " 'and',\n",
       " 'startups',\n",
       " 'are',\n",
       " 'currently',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'developing',\n",
       " 'more',\n",
       " 'powerful',\n",
       " 'processors',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'run',\n",
       " 'today',\n",
       " \"'s\",\n",
       " 'data-intensive',\n",
       " 'ai',\n",
       " 'algorithms.it',\n",
       " 'would',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'vastly',\n",
       " 'more',\n",
       " 'efficient',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'while',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'trained',\n",
       " 'using',\n",
       " 'small',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'are',\n",
       " 'not',\n",
       " 'a',\n",
       " 'reality',\n",
       " 'yet',\n",
       " ',',\n",
       " 'research',\n",
       " 'is',\n",
       " 'already',\n",
       " 'being',\n",
       " 'done',\n",
       " 'on',\n",
       " 'making',\n",
       " 'algorithms',\n",
       " 'smaller',\n",
       " 'without',\n",
       " 'losing',\n",
       " 'accuracy',\n",
       " ',',\n",
       " 'bill',\n",
       " 'dally',\n",
       " ',',\n",
       " 'chief',\n",
       " 'scientist',\n",
       " 'at',\n",
       " 'nvidia',\n",
       " ',',\n",
       " 'said',\n",
       " 'at',\n",
       " 'the',\n",
       " 'conference.nvidia',\n",
       " 'researchers',\n",
       " 'use',\n",
       " 'a',\n",
       " 'process',\n",
       " 'called',\n",
       " 'network',\n",
       " 'pruning',\n",
       " 'to',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'smaller',\n",
       " 'and',\n",
       " 'more',\n",
       " 'efficient',\n",
       " 'to',\n",
       " 'run',\n",
       " 'by',\n",
       " 'removing',\n",
       " 'the',\n",
       " 'neurons',\n",
       " 'that',\n",
       " 'do',\n",
       " 'no',\n",
       " 'contribute',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'output',\n",
       " '.',\n",
       " '``',\n",
       " 'there',\n",
       " 'are',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'training',\n",
       " 'that',\n",
       " 'can',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'complexity',\n",
       " 'of',\n",
       " 'training',\n",
       " 'by',\n",
       " 'huge',\n",
       " 'amounts',\n",
       " \"''\",\n",
       " 'dally',\n",
       " 'said',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "words = word_tokenize(text)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'january',\n",
       " 'google',\n",
       " 'launched',\n",
       " 'a',\n",
       " 'new',\n",
       " 'service',\n",
       " 'called',\n",
       " 'cloud',\n",
       " 'automl',\n",
       " 'which',\n",
       " 'can',\n",
       " 'automate',\n",
       " 'some',\n",
       " 'tricky',\n",
       " 'aspects',\n",
       " 'of',\n",
       " 'designing',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'software',\n",
       " 'while',\n",
       " 'working',\n",
       " 'on',\n",
       " 'this',\n",
       " 'project',\n",
       " 'the',\n",
       " 'company',\n",
       " 's',\n",
       " 'researchers',\n",
       " 'sometimes',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'run',\n",
       " 'as',\n",
       " 'many',\n",
       " 'as',\n",
       " '800',\n",
       " 'graphics',\n",
       " 'chips',\n",
       " 'in',\n",
       " 'unison',\n",
       " 'to',\n",
       " 'train',\n",
       " 'their',\n",
       " 'powerful',\n",
       " 'algorithms',\n",
       " 'unlike',\n",
       " 'humans',\n",
       " 'who',\n",
       " 'can',\n",
       " 'recognize',\n",
       " 'coffee',\n",
       " 'cups',\n",
       " 'from',\n",
       " 'seeing',\n",
       " 'one',\n",
       " 'or',\n",
       " 'two',\n",
       " 'examples',\n",
       " 'ai',\n",
       " 'networks',\n",
       " 'based',\n",
       " 'on',\n",
       " 'simulated',\n",
       " 'neurons',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'tens',\n",
       " 'of',\n",
       " 'thousands',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'identify',\n",
       " 'an',\n",
       " 'object',\n",
       " 'imagine',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'to',\n",
       " 'recognize',\n",
       " 'every',\n",
       " 'item',\n",
       " 'in',\n",
       " 'your',\n",
       " 'environment',\n",
       " 'that',\n",
       " 'way',\n",
       " 'and',\n",
       " 'you',\n",
       " 'begin',\n",
       " 'to',\n",
       " 'understand',\n",
       " 'why',\n",
       " 'ai',\n",
       " 'software',\n",
       " 'requires',\n",
       " 'so',\n",
       " 'much',\n",
       " 'computing',\n",
       " 'power',\n",
       " 'if',\n",
       " 'researchers',\n",
       " 'could',\n",
       " 'design',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'that',\n",
       " 'could',\n",
       " 'be',\n",
       " 'trained',\n",
       " 'to',\n",
       " 'do',\n",
       " 'certain',\n",
       " 'tasks',\n",
       " 'using',\n",
       " 'only',\n",
       " 'a',\n",
       " 'handful',\n",
       " 'of',\n",
       " 'examples',\n",
       " 'it',\n",
       " 'would',\n",
       " 'upend',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'paradigm',\n",
       " 'charles',\n",
       " 'bergan',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'of',\n",
       " 'engineering',\n",
       " 'at',\n",
       " 'qualcomm',\n",
       " 'told',\n",
       " 'the',\n",
       " 'crowd',\n",
       " 'at',\n",
       " 'mit',\n",
       " 'technology',\n",
       " 'review',\n",
       " 's',\n",
       " 'emtech',\n",
       " 'china',\n",
       " 'conference',\n",
       " 'earlier',\n",
       " 'this',\n",
       " 'week',\n",
       " 'if',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'were',\n",
       " 'to',\n",
       " 'become',\n",
       " 'capable',\n",
       " 'of',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'bergan',\n",
       " 'said',\n",
       " 'the',\n",
       " 'cumbersome',\n",
       " 'process',\n",
       " 'of',\n",
       " 'feeding',\n",
       " 'reams',\n",
       " 'of',\n",
       " 'data',\n",
       " 'into',\n",
       " 'algorithms',\n",
       " 'to',\n",
       " 'train',\n",
       " 'them',\n",
       " 'would',\n",
       " 'be',\n",
       " 'rendered',\n",
       " 'obsolete',\n",
       " 'this',\n",
       " 'could',\n",
       " 'have',\n",
       " 'serious',\n",
       " 'consequences',\n",
       " 'for',\n",
       " 'the',\n",
       " 'hardware',\n",
       " 'industry',\n",
       " 'as',\n",
       " 'both',\n",
       " 'existing',\n",
       " 'tech',\n",
       " 'giants',\n",
       " 'and',\n",
       " 'startups',\n",
       " 'are',\n",
       " 'currently',\n",
       " 'focused',\n",
       " 'on',\n",
       " 'developing',\n",
       " 'more',\n",
       " 'powerful',\n",
       " 'processors',\n",
       " 'designed',\n",
       " 'to',\n",
       " 'run',\n",
       " 'today',\n",
       " 's',\n",
       " 'data',\n",
       " 'intensive',\n",
       " 'ai',\n",
       " 'algorithms',\n",
       " 'it',\n",
       " 'would',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'vastly',\n",
       " 'more',\n",
       " 'efficient',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'while',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'that',\n",
       " 'can',\n",
       " 'be',\n",
       " 'trained',\n",
       " 'using',\n",
       " 'small',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'are',\n",
       " 'not',\n",
       " 'a',\n",
       " 'reality',\n",
       " 'yet',\n",
       " 'research',\n",
       " 'is',\n",
       " 'already',\n",
       " 'being',\n",
       " 'done',\n",
       " 'on',\n",
       " 'making',\n",
       " 'algorithms',\n",
       " 'smaller',\n",
       " 'without',\n",
       " 'losing',\n",
       " 'accuracy',\n",
       " 'bill',\n",
       " 'dally',\n",
       " 'chief',\n",
       " 'scientist',\n",
       " 'at',\n",
       " 'nvidia',\n",
       " 'said',\n",
       " 'at',\n",
       " 'the',\n",
       " 'conference',\n",
       " 'nvidia',\n",
       " 'researchers',\n",
       " 'use',\n",
       " 'a',\n",
       " 'process',\n",
       " 'called',\n",
       " 'network',\n",
       " 'pruning',\n",
       " 'to',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'smaller',\n",
       " 'and',\n",
       " 'more',\n",
       " 'efficient',\n",
       " 'to',\n",
       " 'run',\n",
       " 'by',\n",
       " 'removing',\n",
       " 'the',\n",
       " 'neurons',\n",
       " 'that',\n",
       " 'do',\n",
       " 'no',\n",
       " 'contribute',\n",
       " 'directly',\n",
       " 'to',\n",
       " 'output',\n",
       " 'there',\n",
       " 'are',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'training',\n",
       " 'that',\n",
       " 'can',\n",
       " 'reduce',\n",
       " 'the',\n",
       " 'complexity',\n",
       " 'of',\n",
       " 'training',\n",
       " 'by',\n",
       " 'huge',\n",
       " 'amounts',\n",
       " 'dally',\n",
       " 'said']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words_2 = tokenizer.tokenize(text)\n",
    "words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "Looking throught the list, you might see a lot of words that do not bring the real context to the sentece. That is why we will use stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['january',\n",
       " 'google',\n",
       " 'launched',\n",
       " 'new',\n",
       " 'service',\n",
       " 'called',\n",
       " 'cloud',\n",
       " 'automl',\n",
       " 'automate',\n",
       " 'tricky',\n",
       " 'aspects',\n",
       " 'designing',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'software',\n",
       " 'working',\n",
       " 'project',\n",
       " 'company',\n",
       " 'researchers',\n",
       " 'sometimes',\n",
       " 'needed',\n",
       " 'run',\n",
       " 'many',\n",
       " '800',\n",
       " 'graphics',\n",
       " 'chips',\n",
       " 'unison',\n",
       " 'train',\n",
       " 'powerful',\n",
       " 'algorithms',\n",
       " 'unlike',\n",
       " 'humans',\n",
       " 'recognize',\n",
       " 'coffee',\n",
       " 'cups',\n",
       " 'seeing',\n",
       " 'one',\n",
       " 'two',\n",
       " 'examples',\n",
       " 'ai',\n",
       " 'networks',\n",
       " 'based',\n",
       " 'simulated',\n",
       " 'neurons',\n",
       " 'need',\n",
       " 'see',\n",
       " 'tens',\n",
       " 'thousands',\n",
       " 'examples',\n",
       " 'order',\n",
       " 'identify',\n",
       " 'object',\n",
       " 'imagine',\n",
       " 'trying',\n",
       " 'learn',\n",
       " 'recognize',\n",
       " 'every',\n",
       " 'item',\n",
       " 'environment',\n",
       " 'way',\n",
       " 'begin',\n",
       " 'understand',\n",
       " 'ai',\n",
       " 'software',\n",
       " 'requires',\n",
       " 'much',\n",
       " 'computing',\n",
       " 'power',\n",
       " 'researchers',\n",
       " 'could',\n",
       " 'design',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'could',\n",
       " 'trained',\n",
       " 'certain',\n",
       " 'tasks',\n",
       " 'using',\n",
       " 'handful',\n",
       " 'examples',\n",
       " 'would',\n",
       " 'upend',\n",
       " 'whole',\n",
       " 'paradigm',\n",
       " 'charles',\n",
       " 'bergan',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'engineering',\n",
       " 'qualcomm',\n",
       " 'told',\n",
       " 'crowd',\n",
       " 'mit',\n",
       " 'technology',\n",
       " 'review',\n",
       " 'emtech',\n",
       " 'china',\n",
       " 'conference',\n",
       " 'earlier',\n",
       " 'week',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'become',\n",
       " 'capable',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learning',\n",
       " 'bergan',\n",
       " 'said',\n",
       " 'cumbersome',\n",
       " 'process',\n",
       " 'feeding',\n",
       " 'reams',\n",
       " 'data',\n",
       " 'algorithms',\n",
       " 'train',\n",
       " 'would',\n",
       " 'rendered',\n",
       " 'obsolete',\n",
       " 'could',\n",
       " 'serious',\n",
       " 'consequences',\n",
       " 'hardware',\n",
       " 'industry',\n",
       " 'existing',\n",
       " 'tech',\n",
       " 'giants',\n",
       " 'startups',\n",
       " 'currently',\n",
       " 'focused',\n",
       " 'developing',\n",
       " 'powerful',\n",
       " 'processors',\n",
       " 'designed',\n",
       " 'run',\n",
       " 'today',\n",
       " 'data',\n",
       " 'intensive',\n",
       " 'ai',\n",
       " 'algorithms',\n",
       " 'would',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'vastly',\n",
       " 'efficient',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'trained',\n",
       " 'using',\n",
       " 'small',\n",
       " 'data',\n",
       " 'sets',\n",
       " 'reality',\n",
       " 'yet',\n",
       " 'research',\n",
       " 'already',\n",
       " 'done',\n",
       " 'making',\n",
       " 'algorithms',\n",
       " 'smaller',\n",
       " 'without',\n",
       " 'losing',\n",
       " 'accuracy',\n",
       " 'bill',\n",
       " 'dally',\n",
       " 'chief',\n",
       " 'scientist',\n",
       " 'nvidia',\n",
       " 'said',\n",
       " 'conference',\n",
       " 'nvidia',\n",
       " 'researchers',\n",
       " 'use',\n",
       " 'process',\n",
       " 'called',\n",
       " 'network',\n",
       " 'pruning',\n",
       " 'make',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'smaller',\n",
       " 'efficient',\n",
       " 'run',\n",
       " 'removing',\n",
       " 'neurons',\n",
       " 'contribute',\n",
       " 'directly',\n",
       " 'output',\n",
       " 'ways',\n",
       " 'training',\n",
       " 'reduce',\n",
       " 'complexity',\n",
       " 'training',\n",
       " 'huge',\n",
       " 'amounts',\n",
       " 'dally',\n",
       " 'said']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = [w for w in words_2 if not w in stop_words]\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming is used to normalize the words. We have a lot of variations of the same word, carrying the same meaning other than when tense is involved. We stem to shorten up the lookup and normalize sentences. Keep in mind that this method can often create non-existent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['januari',\n",
       " 'googl',\n",
       " 'launch',\n",
       " 'new',\n",
       " 'servic',\n",
       " 'call',\n",
       " 'cloud',\n",
       " 'automl',\n",
       " 'autom',\n",
       " 'tricki',\n",
       " 'aspect',\n",
       " 'design',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'softwar',\n",
       " 'work',\n",
       " 'project',\n",
       " 'compani',\n",
       " 'research',\n",
       " 'sometim',\n",
       " 'need',\n",
       " 'run',\n",
       " 'mani',\n",
       " '800',\n",
       " 'graphic',\n",
       " 'chip',\n",
       " 'unison',\n",
       " 'train',\n",
       " 'power',\n",
       " 'algorithm',\n",
       " 'unlik',\n",
       " 'human',\n",
       " 'recogn',\n",
       " 'coffe',\n",
       " 'cup',\n",
       " 'see',\n",
       " 'one',\n",
       " 'two',\n",
       " 'exampl',\n",
       " 'ai',\n",
       " 'network',\n",
       " 'base',\n",
       " 'simul',\n",
       " 'neuron',\n",
       " 'need',\n",
       " 'see',\n",
       " 'ten',\n",
       " 'thousand',\n",
       " 'exampl',\n",
       " 'order',\n",
       " 'identifi',\n",
       " 'object',\n",
       " 'imagin',\n",
       " 'tri',\n",
       " 'learn',\n",
       " 'recogn',\n",
       " 'everi',\n",
       " 'item',\n",
       " 'environ',\n",
       " 'way',\n",
       " 'begin',\n",
       " 'understand',\n",
       " 'ai',\n",
       " 'softwar',\n",
       " 'requir',\n",
       " 'much',\n",
       " 'comput',\n",
       " 'power',\n",
       " 'research',\n",
       " 'could',\n",
       " 'design',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'could',\n",
       " 'train',\n",
       " 'certain',\n",
       " 'task',\n",
       " 'use',\n",
       " 'hand',\n",
       " 'exampl',\n",
       " 'would',\n",
       " 'upend',\n",
       " 'whole',\n",
       " 'paradigm',\n",
       " 'charl',\n",
       " 'bergan',\n",
       " 'vice',\n",
       " 'presid',\n",
       " 'engin',\n",
       " 'qualcomm',\n",
       " 'told',\n",
       " 'crowd',\n",
       " 'mit',\n",
       " 'technolog',\n",
       " 'review',\n",
       " 'emtech',\n",
       " 'china',\n",
       " 'confer',\n",
       " 'earlier',\n",
       " 'week',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'becom',\n",
       " 'capabl',\n",
       " 'one',\n",
       " 'shot',\n",
       " 'learn',\n",
       " 'bergan',\n",
       " 'said',\n",
       " 'cumbersom',\n",
       " 'process',\n",
       " 'feed',\n",
       " 'ream',\n",
       " 'data',\n",
       " 'algorithm',\n",
       " 'train',\n",
       " 'would',\n",
       " 'render',\n",
       " 'obsolet',\n",
       " 'could',\n",
       " 'seriou',\n",
       " 'consequ',\n",
       " 'hardwar',\n",
       " 'industri',\n",
       " 'exist',\n",
       " 'tech',\n",
       " 'giant',\n",
       " 'startup',\n",
       " 'current',\n",
       " 'focus',\n",
       " 'develop',\n",
       " 'power',\n",
       " 'processor',\n",
       " 'design',\n",
       " 'run',\n",
       " 'today',\n",
       " 'data',\n",
       " 'intens',\n",
       " 'ai',\n",
       " 'algorithm',\n",
       " 'would',\n",
       " 'also',\n",
       " 'mean',\n",
       " 'vastli',\n",
       " 'effici',\n",
       " 'machin',\n",
       " 'learn',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'train',\n",
       " 'use',\n",
       " 'small',\n",
       " 'data',\n",
       " 'set',\n",
       " 'realiti',\n",
       " 'yet',\n",
       " 'research',\n",
       " 'alreadi',\n",
       " 'done',\n",
       " 'make',\n",
       " 'algorithm',\n",
       " 'smaller',\n",
       " 'without',\n",
       " 'lose',\n",
       " 'accuraci',\n",
       " 'bill',\n",
       " 'dalli',\n",
       " 'chief',\n",
       " 'scientist',\n",
       " 'nvidia',\n",
       " 'said',\n",
       " 'confer',\n",
       " 'nvidia',\n",
       " 'research',\n",
       " 'use',\n",
       " 'process',\n",
       " 'call',\n",
       " 'network',\n",
       " 'prune',\n",
       " 'make',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'smaller',\n",
       " 'effici',\n",
       " 'run',\n",
       " 'remov',\n",
       " 'neuron',\n",
       " 'contribut',\n",
       " 'directli',\n",
       " 'output',\n",
       " 'way',\n",
       " 'train',\n",
       " 'reduc',\n",
       " 'complex',\n",
       " 'train',\n",
       " 'huge',\n",
       " 'amount',\n",
       " 'dalli',\n",
       " 'said']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmered_words = [ps.stem(w) for w in words_2 if not w in stop_words]\n",
    "stemmered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling words\n",
    "This is one of the most powerfull parts if NLTK module. It can label words in a sentence as nouns, adjectives, verbs...etc.\n",
    "When we had tokenized and stemmed the text now lets build function that will tag the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/konradb/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[('januari', 'NN')]\n",
      "[('googl', 'NN')]\n",
      "[('launch', 'NN')]\n",
      "[('new', 'JJ')]\n",
      "[('servic', 'NN')]\n",
      "[('call', 'NN')]\n",
      "[('cloud', 'NN')]\n",
      "[('automl', 'NN')]\n",
      "[('autom', 'NN')]\n",
      "[('tricki', 'NN')]\n",
      "[('aspect', 'NN')]\n",
      "[('design', 'NN')]\n",
      "[('machin', 'NN')]\n",
      "[('learn', 'NN')]\n",
      "[('softwar', 'NN')]\n",
      "[('work', 'NN')]\n",
      "[('project', 'NN')]\n",
      "[('compani', 'NN')]\n",
      "[('research', 'NN')]\n",
      "[('sometim', 'NN')]\n",
      "[('need', 'NN')]\n",
      "[('run', 'VB')]\n",
      "[('mani', 'NN')]\n",
      "[('800', 'CD')]\n",
      "[('graphic', 'JJ')]\n",
      "[('chip', 'NN')]\n",
      "[('unison', 'NN')]\n",
      "[('train', 'NN')]\n",
      "[('power', 'NN')]\n",
      "[('algorithm', 'NN')]\n",
      "[('unlik', 'NN')]\n",
      "[('human', 'NN')]\n",
      "[('recogn', 'NN')]\n",
      "[('coffe', 'NN')]\n",
      "[('cup', 'NN')]\n",
      "[('see', 'VB')]\n",
      "[('one', 'CD')]\n",
      "[('two', 'CD')]\n",
      "[('exampl', 'NN')]\n",
      "[('ai', 'NN')]\n",
      "[('network', 'NN')]\n",
      "[('base', 'NN')]\n",
      "[('simul', 'NN')]\n",
      "[('neuron', 'NN')]\n",
      "[('need', 'NN')]\n",
      "[('see', 'VB')]\n",
      "[('ten', 'NNS')]\n",
      "[('thousand', 'NN')]\n",
      "[('exampl', 'NN')]\n",
      "[('order', 'NN')]\n",
      "[('identifi', 'NN')]\n",
      "[('object', 'NN')]\n",
      "[('imagin', 'NN')]\n",
      "[('tri', 'NN')]\n",
      "[('learn', 'NN')]\n",
      "[('recogn', 'NN')]\n",
      "[('everi', 'NN')]\n",
      "[('item', 'NN')]\n",
      "[('environ', 'NN')]\n",
      "[('way', 'NN')]\n",
      "[('begin', 'NN')]\n",
      "[('understand', 'NN')]\n",
      "[('ai', 'NN')]\n",
      "[('softwar', 'NN')]\n",
      "[('requir', 'NN')]\n",
      "[('much', 'JJ')]\n",
      "[('comput', 'NN')]\n",
      "[('power', 'NN')]\n",
      "[('research', 'NN')]\n",
      "[('could', 'MD')]\n",
      "[('design', 'NN')]\n",
      "[('neural', 'JJ')]\n",
      "[('network', 'NN')]\n",
      "[('could', 'MD')]\n",
      "[('train', 'NN')]\n",
      "[('certain', 'JJ')]\n",
      "[('task', 'NN')]\n",
      "[('use', 'NN')]\n",
      "[('hand', 'NN')]\n",
      "[('exampl', 'NN')]\n",
      "[('would', 'MD')]\n",
      "[('upend', 'NN')]\n",
      "[('whole', 'JJ')]\n",
      "[('paradigm', 'NN')]\n",
      "[('charl', 'NN')]\n",
      "[('bergan', 'NN')]\n",
      "[('vice', 'NN')]\n",
      "[('presid', 'NN')]\n",
      "[('engin', 'NN')]\n",
      "[('qualcomm', 'NN')]\n",
      "[('told', 'NN')]\n",
      "[('crowd', 'NN')]\n",
      "[('mit', 'NN')]\n",
      "[('technolog', 'NN')]\n",
      "[('review', 'NN')]\n",
      "[('emtech', 'NN')]\n",
      "[('china', 'NN')]\n",
      "[('confer', 'NN')]\n",
      "[('earlier', 'RBR')]\n",
      "[('week', 'NN')]\n",
      "[('neural', 'JJ')]\n",
      "[('network', 'NN')]\n",
      "[('becom', 'NN')]\n",
      "[('capabl', 'NN')]\n",
      "[('one', 'CD')]\n",
      "[('shot', 'NN')]\n",
      "[('learn', 'NN')]\n",
      "[('bergan', 'NN')]\n",
      "[('said', 'VBD')]\n",
      "[('cumbersom', 'NN')]\n",
      "[('process', 'NN')]\n",
      "[('feed', 'NN')]\n",
      "[('ream', 'NN')]\n",
      "[('data', 'NNS')]\n",
      "[('algorithm', 'NN')]\n",
      "[('train', 'NN')]\n",
      "[('would', 'MD')]\n",
      "[('render', 'NN')]\n",
      "[('obsolet', 'NN')]\n",
      "[('could', 'MD')]\n",
      "[('seriou', 'NN')]\n",
      "[('consequ', 'NN')]\n",
      "[('hardwar', 'NN')]\n",
      "[('industri', 'NN')]\n",
      "[('exist', 'NN')]\n",
      "[('tech', 'NN')]\n",
      "[('giant', 'NN')]\n",
      "[('startup', 'NN')]\n",
      "[('current', 'JJ')]\n",
      "[('focus', 'NN')]\n",
      "[('develop', 'VB')]\n",
      "[('power', 'NN')]\n",
      "[('processor', 'NN')]\n",
      "[('design', 'NN')]\n",
      "[('run', 'VB')]\n",
      "[('today', 'NN')]\n",
      "[('data', 'NNS')]\n",
      "[('intens', 'NNS')]\n",
      "[('ai', 'NN')]\n",
      "[('algorithm', 'NN')]\n",
      "[('would', 'MD')]\n",
      "[('also', 'RB')]\n",
      "[('mean', 'NN')]\n",
      "[('vastli', 'NN')]\n",
      "[('effici', 'NN')]\n",
      "[('machin', 'NN')]\n",
      "[('learn', 'NN')]\n",
      "[('neural', 'JJ')]\n",
      "[('network', 'NN')]\n",
      "[('train', 'NN')]\n",
      "[('use', 'NN')]\n",
      "[('small', 'JJ')]\n",
      "[('data', 'NNS')]\n",
      "[('set', 'NN')]\n",
      "[('realiti', 'NN')]\n",
      "[('yet', 'RB')]\n",
      "[('research', 'NN')]\n",
      "[('alreadi', 'NN')]\n",
      "[('done', 'VBN')]\n",
      "[('make', 'VB')]\n",
      "[('algorithm', 'NN')]\n",
      "[('smaller', 'JJR')]\n",
      "[('without', 'IN')]\n",
      "[('lose', 'VB')]\n",
      "[('accuraci', 'NN')]\n",
      "[('bill', 'NN')]\n",
      "[('dalli', 'NN')]\n",
      "[('chief', 'NN')]\n",
      "[('scientist', 'NN')]\n",
      "[('nvidia', 'NN')]\n",
      "[('said', 'VBD')]\n",
      "[('confer', 'NN')]\n",
      "[('nvidia', 'NN')]\n",
      "[('research', 'NN')]\n",
      "[('use', 'NN')]\n",
      "[('process', 'NN')]\n",
      "[('call', 'NN')]\n",
      "[('network', 'NN')]\n",
      "[('prune', 'NN')]\n",
      "[('make', 'VB')]\n",
      "[('neural', 'JJ')]\n",
      "[('network', 'NN')]\n",
      "[('smaller', 'JJR')]\n",
      "[('effici', 'NN')]\n",
      "[('run', 'VB')]\n",
      "[('remov', 'NN')]\n",
      "[('neuron', 'NN')]\n",
      "[('contribut', 'NN')]\n",
      "[('directli', 'NN')]\n",
      "[('output', 'NN')]\n",
      "[('way', 'NN')]\n",
      "[('train', 'NN')]\n",
      "[('reduc', 'NN')]\n",
      "[('complex', 'JJ')]\n",
      "[('train', 'NN')]\n",
      "[('huge', 'JJ')]\n",
      "[('amount', 'NN')]\n",
      "[('dalli', 'NN')]\n",
      "[('said', 'VBD')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in stemmered_words:\n",
    "            words_3 = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words_3)\n",
    "            print(tagged)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "            \n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S in/IN)\n",
      "(S january/NN)\n",
      "(S google/NN)\n",
      "(S launched/VBN)\n",
      "(S a/DT)\n",
      "(S new/JJ)\n",
      "(S service/NN)\n",
      "(S called/VBN)\n",
      "(S cloud/NN)\n",
      "(S automl/NN)\n",
      "(S which/WDT)\n",
      "(S can/MD)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-795ff85fecae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprocess_content_and_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-795ff85fecae>\u001b[0m in \u001b[0;36mprocess_content_and_draw\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mchunked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konradb/anaconda3/lib/python3.6/site-packages/nltk/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \"\"\"\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konradb/anaconda3/lib/python3.6/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \"\"\"\n\u001b[0;32m--> 863\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konradb/anaconda3/lib/python3.6/site-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \"\"\"\n\u001b[1;32m    853\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0min_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/konradb/anaconda3/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Do not work\n",
    "def process_content_and_draw():\n",
    "    try:\n",
    "        for i in words_2:\n",
    "            words_3 = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words_3)\n",
    "            chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "            chunkParser = nltk.RegexpParser(chunkGram)\n",
    "            chunked = chunkParser.parse(tagged)\n",
    "            \n",
    "            print(chunked)\n",
    "            for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "                print(subtree)\n",
    "\n",
    "            chunked.draw()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        \n",
    "process_content_and_draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "\n",
    "Usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma . If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source.\n",
    "[source](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove words that appear only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequency = defaultdict(int)\n",
    "\n",
    "for token in filtered_sentence:\n",
    "    frequency[token] += 1\n",
    "        \n",
    "\n",
    "texts = [token for token in filtered_sentence if frequency[token] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['called',\n",
      " 'machine',\n",
      " 'learning',\n",
      " 'software',\n",
      " 'researchers',\n",
      " 'run',\n",
      " 'train',\n",
      " 'powerful',\n",
      " 'algorithms',\n",
      " 'recognize',\n",
      " 'one',\n",
      " 'examples',\n",
      " 'ai',\n",
      " 'networks',\n",
      " 'neurons',\n",
      " 'examples',\n",
      " 'recognize',\n",
      " 'ai',\n",
      " 'software',\n",
      " 'researchers',\n",
      " 'could',\n",
      " 'neural',\n",
      " 'networks',\n",
      " 'could',\n",
      " 'trained',\n",
      " 'using',\n",
      " 'examples',\n",
      " 'would',\n",
      " 'bergan',\n",
      " 'conference',\n",
      " 'neural',\n",
      " 'networks',\n",
      " 'one',\n",
      " 'learning',\n",
      " 'bergan',\n",
      " 'said',\n",
      " 'process',\n",
      " 'data',\n",
      " 'algorithms',\n",
      " 'train',\n",
      " 'would',\n",
      " 'could',\n",
      " 'powerful',\n",
      " 'run',\n",
      " 'data',\n",
      " 'ai',\n",
      " 'algorithms',\n",
      " 'would',\n",
      " 'efficient',\n",
      " 'machine',\n",
      " 'learning',\n",
      " 'neural',\n",
      " 'networks',\n",
      " 'trained',\n",
      " 'using',\n",
      " 'data',\n",
      " 'algorithms',\n",
      " 'smaller',\n",
      " 'dally',\n",
      " 'nvidia',\n",
      " 'said',\n",
      " 'conference',\n",
      " 'nvidia',\n",
      " 'researchers',\n",
      " 'process',\n",
      " 'called',\n",
      " 'network',\n",
      " 'neural',\n",
      " 'network',\n",
      " 'smaller',\n",
      " 'efficient',\n",
      " 'run',\n",
      " 'neurons',\n",
      " 'training',\n",
      " 'training',\n",
      " 'dally',\n",
      " 'said']\n"
     ]
    }
   ],
   "source": [
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary\n",
    "\n",
    "[Bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) also known as vector space model, in this model a text is represented as the bag (multiset) of its words, disregarding grammar and word order but keeping multiplicity. So we are converting documents to vectors, where each vector is pair question-answer in style of:\n",
    "```\n",
    "“How many times does the word system appear in the document? Once.”\n",
    "```\n",
    "It is usefull to represent questions by their IDs. The mapping between questions and IDs is called a dictionary. So we will assign to each of the tokens (words) unique integer ID. In the end we see 30 distinc words in the processed corpus. We also save the dict and we will see if the words occur in other document as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai': 0,\n",
       " 'algorithms': 1,\n",
       " 'bergan': 2,\n",
       " 'called': 3,\n",
       " 'conference': 4,\n",
       " 'could': 5,\n",
       " 'dally': 6,\n",
       " 'data': 7,\n",
       " 'efficient': 8,\n",
       " 'examples': 9,\n",
       " 'learning': 10,\n",
       " 'machine': 11,\n",
       " 'network': 12,\n",
       " 'networks': 13,\n",
       " 'neural': 14,\n",
       " 'neurons': 15,\n",
       " 'nvidia': 16,\n",
       " 'one': 17,\n",
       " 'powerful': 18,\n",
       " 'process': 19,\n",
       " 'recognize': 20,\n",
       " 'researchers': 21,\n",
       " 'run': 22,\n",
       " 'said': 23,\n",
       " 'smaller': 24,\n",
       " 'software': 25,\n",
       " 'train': 26,\n",
       " 'trained': 27,\n",
       " 'training': 28,\n",
       " 'using': 29,\n",
       " 'would': 30}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary([texts])\n",
    "dictionary.save('/tmp/article_about_ai_dictionary.dict')\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating your own dictionary\n",
    "\n",
    "I have created own dictionary with words that may occure in AI article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ai_dict = {'ai': 0,\n",
    "           'algorithms': 1,\n",
    "           'conference': 2,\n",
    "           'could': 3,\n",
    "           'data': 4,\n",
    "           'data science': 5, \n",
    "           'learning': 6,\n",
    "           'machine': 7,\n",
    "           'network': 8,\n",
    "           'networks': 9,\n",
    "           'backpropagation': 10,\n",
    "           'cnn': 11,\n",
    "           'rnn': 12,\n",
    "           'deep': 13,\n",
    "           'neural': 14,\n",
    "           'neurons': 15,\n",
    "           'nvidia': 16,\n",
    "           'heuristic': 17,\n",
    "           'nlp': 18,\n",
    "           'python': 19,\n",
    "           'recognize': 20,\n",
    "           'researchers': 21,\n",
    "           'predict': 22,\n",
    "           'classification': 23,\n",
    "           'watson': 24,\n",
    "           'software': 25,\n",
    "           'train': 26,\n",
    "           'trained': 27,\n",
    "           'training': 28,\n",
    "           'robotics': 29,\n",
    "           'rule-based': 30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions\n",
    "\n",
    "There are built functions that speed up this proccess. Below there is also example of using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(string_doc):\n",
    "    '''Please pass string document'''\n",
    "    \n",
    "    text_test = string_doc.lower()\n",
    "    words_test = word_tokenize(text_test)\n",
    "    tokenizer_test = RegexpTokenizer(r'\\w+')\n",
    "    words_tokenized_test = tokenizer.tokenize(text_test)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_sentence_test = [w for w in words_tokenized_test if not w in stop_words]\n",
    "    frequency_test = defaultdict(int)\n",
    "    for token in filtered_sentence_test:\n",
    "        frequency_test[token] += 1\n",
    "    \n",
    "    list_frequent_test = [token for token in filtered_sentence if frequency[token] > 1]\n",
    "        \n",
    "    return list_frequent_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_ai_dict(list_of_words):\n",
    "    '''Pass list of words with frequency greater than 1 \n",
    "    and without stop words. Preferably use previous function'''\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for token in list_of_words:\n",
    "        if token in ai_dict.keys():\n",
    "            counter = counter +1\n",
    "    return int(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I make assumption that when article is about AI about at least 50% of the words will be about the topic. So the number of occurances in ```ai_dict``` divided by length of the list of the unique words in article will give us more than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise_article(occurance, list_of_words):\n",
    "    rate = occurance/len(list_of_words)\n",
    "    if rate >= 0.5:\n",
    "        print(\"This article is about AI\")\n",
    "    else:\n",
    "        print(\"This article is not about AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_article(text):\n",
    "    list_2 = process_text(text)\n",
    "    number_of_occurances = compare_with_ai_dict(list_2)\n",
    "    categorise_article(number_of_occurances, list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "So I prepared another article about AI. We will check if the program will answer it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('second_text.txt', 'r') as f:\n",
    "    new_document=f.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw test (step by step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "list_2 = process_text(new_document)\n",
    "number_of_occurances = compare_with_ai_dict(list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6103896103896104"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_occurances/len(list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article is about AI\n"
     ]
    }
   ],
   "source": [
    "categorise_article(number_of_occurances, list_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article is about AI\n"
     ]
    }
   ],
   "source": [
    "check_article(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
